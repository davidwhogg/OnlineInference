% This document is part of the Online Inference project.
% Copyright 2012 David W. Hogg (NYU).

\documentclass[12pt]{article}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}
\newcommand{\info}{I}
\newcommand{\pars}{\theta}
\newcommand{\sample}[2]{\pars^{({#1})}_{#2}}
\newcommand{\datum}[1]{D_{#1}}
\newcommand{\dataset}[1]{\{D_i\}_{i=1}^{#1}}

\begin{document}

\noindent\textsl{[This is a draft document by David W. Hogg dated 2012-04-22.]}

\section{Generalities}

An experiment has produced $N$ data points (or just ``data'')
$\datum{n}$, with $1<n<N$.  In many applications it makes sense to
have the index $n$ \emph{not} run in any sensible, heuristic order
(such as time order); you might want the $n$ indices to be the
labeling of a random permutation of the data.  In other
applications---for example adaptive experimental control
situations---you need the data to be in time order.

The model for the data consists of some blob of assumptions and
knowledge $\info$ (the \emph{prior information}, it includes
noise-model information), a list, vector, or blob of parameters
$\pars$, a \emph{likelihood function} $p(\datum{n}\given\pars,\info)$
that returns the PDF for datum $\datum{n}$ given the parameters and
the prior information, and a prior PDF $p(\pars\given\info)$ that
represents information about the parameters prior to the arrival of
any of the data (from this experiment).  We will assume for simplicity
that the data are \emph{independently drawn}, but we will \emph{not}
assume that they are \emph{iid}; that is, each data point can have
its own individual noise properties, missing dimensions, and
likelihood formulation, in general.

One note that is useful for what follows: The rules of probability
permit us to compute, for each individual data point $\datum{n}$, a
marginalized likelihood
\begin{eqnarray}\displaystyle
p(\datum{n}\given\info) &\equiv& \int p(\datum{1}\given\pars,\info)\,p(\pars\given\info)\,\dd\pars
\quad .
\end{eqnarray}

\section{Online inference}

Start with a prior PDF $p(\pars\given\info)$ and a set of $K$ samples
$\sample{0}{k}$, with $1<k<K$.  The index $(0)$ indicates that this is
the zeroth step of the online inference.

At the first step, take the first data point $\datum{1}$ and compute
approximate marginalized likelihood $Q_1$
\begin{eqnarray}\displaystyle
Q_1 &\leftarrow& \frac{1}{K}\,\sum_{k=1}^K p(\datum{1}\given\sample{0}{k},\info)
\\
Q_1 &\approx& p(\datum{1}\given\info)
\quad .
\end{eqnarray}
This is a sampling-based approximation to the marginalized likelihood.

Use the $p(\datum{1}\given\sample{0}{k},\info)$ to \emph{importance
  sample} [MAKE THIS EXPLICIT] the sampling $\sample{0}{k}$ to a
smaller set of samples (not all $K$ samples will survive the
importance sampling).  The importance sampling will leave behind a
representative sampling of the \emph{posterior probability}
\begin{eqnarray}\displaystyle
p(\pars\given\datum{1},\info) &\equiv& \frac{1}{Z_1}\,p(\datum{1}\given\pars,\info)\,p(\pars\given\info)
\\
Z_1 &\equiv& p(\datum{1}\given\info)
\quad .
\end{eqnarray}
Use this small importance-sampling sample to initialize an MCMC that
grows the sampling back up to a full set of $K$ samples
$\sample{1}{k}$, where the $(1)$ indicates that the sampling is
appropriate for the posterior PDF generated by the first data point.

The key insight of online inference is that the posterior PDF at the
end of this process run on the first datum $\datum{1}$ is the
\emph{prior PDF} for the second datum $\datum{2}$.  Thus, the iterated
procedure is as follows:

At the $n$th step:
\begin{enumerate}
\item You begin with a set of $K$ samples $\sample{n-1}{k}$ from the
  relevant prior PDF, which is really
  $p(\pars\given\dataset{n-1},\info)$ (the posterior PDF from the
  previous step).
\item Compute and save
  \begin{eqnarray}\displaystyle
    Q_n &\leftarrow& \frac{1}{K}\,\sum_{k=1}^K p(\datum{n}\given\sample{n-1}{k},\info)
    \quad .
  \end{eqnarray}
\item By a combination of importance sampling and MCMC generate a new
  set of $K$ samples $\sample{n}{k}$ from the new posterior PDF
  \begin{eqnarray}\displaystyle
    p(\pars\given\dataset{n},\info) &\equiv& \frac{1}{Z_n}\,p(\datum{n}\given\pars,\info)\,p(\pars\given\dataset{n-1},\info)
  \quad .
  \end{eqnarray}
\end{enumerate}
Iteration of this inference operation, once for each datum $\datum{n}$,
leads to a set of $N$ approximate estimates $Q_n$ of the marginalized
likelihoods and a final set of $K$ samples $\sample{N}{k}$ that ought
to represent a fair sampling from the posterior PDF for the parameters
given the full dataset $\dataset{N}$.

IIRC, an estimate of the full marginalized likelihood can be obtained
by producting together the individual estimates, or
\begin{eqnarray}\displaystyle
\prod_{n=1}^N Q_n &\approx& \int p(\dataset{N}\given\pars,\info)\,p(\pars\given\info)\,\dd\pars
\quad.
\end{eqnarray}
I certainly hope that---or something like it---is true.

\section{Application and results}

...Exoplanet example?..

...Do we get good answers, and does it all work?..

...What are the dependences of the results on $K$ and on the data ordering?..

...Related to that: performance metrics vs other samplers...

\section{Discussion}

...This method has some close relationship with thermodynamic
integration or tempering or nested sampling, because the slow
introduction of data looks numerically a lot like slow reduction of
the temperature...

\end{document}
